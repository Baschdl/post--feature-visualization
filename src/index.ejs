<!doctype html>
<script src="template.v2.js"></script>
<d-front-matter>
  <script type="text/json">{
  "title": "Article Title",
  "description": "Description of the post",
  "authors": [
    {
      "author":"Chris Olah",
      "authorURL":"https://colah.github.io/",
      "affiliation":"Google Brain",
      "affiliationURL":"https://g.co/brain"
    },
    {
      "author":"Chris Olah",
      "authorURL":"https://colah.github.io/",
      "affiliation":"Google Brain",
      "affiliationURL":"https://g.co/brain"
    },
    {
      "author":"Chris Olah",
      "authorURL":"https://colah.github.io/",
      "affiliation":"Google Brain",
      "affiliationURL":"https://g.co/brain"
    }
  ]
  }</script>
</d-front-matter>

<style>
  .optimize-nojitter-sprite {
    background-image: url("images/optimize_nojitter_sprite_128.jpeg");
  }
  .optimize-jitter-sprite {
    background-image: url("images/optimize_jitter_sprite_128.jpeg");
  }
  .steepst-dir-sprite {
    background-image: url("images/steepest_dirs_sprite_224.jpeg");
  }
  d-fn {
    line-height: 100%;
  }
  .example-vis {
    display: flex;
    flex-wrap: wrap;
    flex-flow: row;
    /*align-items: center;*/
    justify-content: space-between;
    align-items: flex-start;
    min-height: 240px;
  }
  .example-vis > .chunk {
    /*width: calc(3 * 64px + 3 * 2px);*/
    display: -ms-flexbox;
    display: -webkit-flex;
    display: flex;
    flex-flow: column;
    flex-wrap: wrap;
    align-items: flex-start;
    /*--padding-right: 46px;*/
    /*padding-right: 24px;*/
  }
  .example-vis > .chunk:last-child {
    padding-right: 0px;
  }
  .example-vis > .chunk > img {
    width: 64px;
    height: 64px;
    padding-right: 1px;
    padding-bottom: 1px;
    display: block;
  }
  .example-vis > .chunk > img:first {
    padding-bottom: 10px;
  }
  .example-vis > .chunk > .big {
    width: 147px;
    height: 147px;
  }
  .example-vis > .chunk > figcaption {
    margin-top: 15px;
  }

  .example-vis2 {
    display: -ms-flexbox;
    display: -webkit-flex;
    display: flex;
    flex-wrap: wrap;
    /*align-items: center;*/
    justify-content: flex-start;
    align-items: flex-start;
  }

  .example-vis2 > .chunk > img {
    width: 170px;
    height: 170px;
    padding-right: 10px;
  }
  .example-vis2 > .chunk > figcaption {
    margin-top: 5px;
  }
  
  #googlenet-examples {
    grid-column: left / text;
  }
  @media (min-width: 1105px) {
    #googlenet-examples {
      grid-column: start / end;
    }
  }
</style>






  <!--
  * What is the one sentence description of this piece?
  ** Neural network feature visualization is a powerful technique, but is often complicated. We find that simple techniques can produce state-of-the-art visualizations -- and that these visualizations are, surprisingly, often limited by optimization problems.
-->
   



<d-article>
  
  <h1>Simple &amp; Flexible Feature Visualization</h1>
  <h2>Feature visualization allows us to see how neural networks progressively build up their understanding of images.</h2>
  
  <figure id="googlenet-examples" style="min-height: 438px;"></figure>
  <d-byline></d-byline>

  <!-- <aside>TODO: heroify! TODO: Could this visualization “expand” to let the user see more images? We should also link to lots more stuff here!!</aside> -->
  
  <!-- <d-abstract>The best methods can be a bit complicated -- but we find that simple techniques can be very effective, and that one of the main challenges may be optimization.</d-abstract> -->
  <d-abstract>Neural network feature visualization is a powerful technique, but is often complicated. We find that simple techniques can produce state-of-the-art visualizations -- and that these visualizations are, surprisingly, often limited by optimization problems.</d-abstract>
  <!-- <d-toc></d-toc> -->

  <p>What does this neuron mean? What does my network understand a dog to look like? Feature visualization answers these questions by generating examples of what a neuron is looking for.  We’ve had a great deal of success at doing this, but the best results have generally involved complicated priors, which can make these methods a bit finicky and their meaning unclear. 

  <p>It turns out that we can make compelling feature visualizations with a very simple method -- the complicated priors seem to not be necessary!  This method can show us how neural networks used for vision progressively build up their understanding of images:


  <p>It can also…

  Show us class visualizations
  Audio model????

  <p>But how does this new technique for creating feature visualizations work?

<!--

  <h2>Optimization & Preconditioners</h2>

  <p>Neural networks are, generally speaking, differentiable with respect to their inputs. If we want to change the behavior of the network -- whether that’s an internal neuron firing or the final output -- we can take the derivatives to figure out how to tweak the input in order to move towards that goal.

  <figure id="optimize-naive_old" class="l-page-outset"></figure>

  <p>If you simply optimize the images to get neurons to fire  as above, the images get filled by checkerboard artifacts and other high frequency patterns. These patterns seem to be the images kind of cheating by exploiting their alignment with strided convolutions and pooling operations.

  <p>The solution to this is to optimize for an image that robustly activates the network. The most common solution is to jitter the image, shifting the way the image exactly aligns with the network. If we do this, we get much better examples:

  <figure id="optimize-jitter_old" class="l-page-outset"></figure>

  <p>While this is certainly better than before, it’s still slow and we continue to see significant changes if we really crank the optimization crank, exponentially increasing the training time. Continuing this increase isn’t sustainable, but it makes one wonder what would happen if we could optimize much better. 

  <p>If one thinks about it, turning random noise into a good image seems like it could be a hard optimization problem. Creating large scale structure requires us to move many pixels in concert. Achieving particular colors requires coordination between RGB channels. These kind of relationships are a recipe for pathological curvature and tricky optimization. 

  <p>So, could it be that we’re running into a hard optimization problem? Well, it seems plausible and a few things point in that direction. If we look at the gradient, it’s dominated by high spatial frequencies -- there’s very little movement in the lower frequency directions that would be essential for creating larger scale structure. Of course, the results keep improving, if very slowly, as we continue optimizing seems like a pretty big clue.

  <p>When we run into optimization problems, there are a plethora of powerful tools one can bring to bare, but an easy starting point is to try whitening transformations. Many optimization problems become a lot easier if we can find a change of basis that makes the variables being optimized more independent. In the case of images, decorrelating or “whitening” them is a well studied problem, with well known transformations for natural images that we can simply apply.

  <p>[vis]

  <p>Of course, transformations that decorrelate image space as a whole may not be the right transformation to decorrelate the local curvature of our optimization problem. Ideally, we’d use a second order 



  <p>High-frequency gradients
  <span>Orders of magnitude training time

  <figure id="opt-explore" class="l-page-outset"></figure>
  
-->
  
  
  

<!-- =================================================== -->
<h2>Feature Visualization by Optimization</h2>
<p><i>
  There's a growing sense that neural networks need to be interpretable.
  Optimization based feature visualization is one of our best tools to date.
</i></p>
<br>

<p>
  Neural networks are, generally speaking, differentiable with respect to their inputs.
  If we want to find out what kind of input would cause a certain behavior
  -- whether that’s an internal neuron or the final output firing --
  we can use derivatives to figure out how to iteratively tweak the input
  towards that goal <d-cite key="erhan2009visualizing"></d-cite>.
</p>

<figure class="l-body" style="min-height: 150px;">
  <img style="width: 100%; " src="images/vis_OptimizeProcess.svg">
  <figcaption>Starting with random noise, we optimize an image to activate a particular neuron (layer mixed4a, unit 11).</figcaption>
</figure>

<h3>Objectives</h3>

<p>
  - TODO: draft of first feature viz article
</p>

<h3>Why visualize by optimization?</h3>

<p>
  So, optimization can give us an example input that causes the desired behavior.
  But why bother with that?
  Couldn't we just look through the dataset for examples that cause the desired behavior?
</p>

<p>
  This optimization approach can be a powerful way to see what a model is really looking for, because it separates the things causing behavior from from things that merely correlate with the causes. For example, consider the following neurons visualized with dataset examples and optimization:
</p>

<style>
  #example-optimization-comparison {
    
  }
  
  #example-optimization-comparison .row {
    justify-content: start;
  }
  
  #example-optimization-comparison .row > * {
    padding-right: 15px;
    width: 147px;
  }
  #example-optimization-comparison .row > img {
    height: 147px;
    padding-bottom: 10px;
  }
  
  @media (min-width: 960px) {
    #example-optimization-comparison .row > * {
      padding-right: 20px;
      width: 180px;
    }
    #example-optimization-comparison .row > img {
      height: 180px;
      padding-bottom: 10px;
    }
  }
</style>

<figure class="l-page-outset" id="example-optimization-comparison">
  <div class="row">
    <img src="https://cnsviewer2.corp.google.com/cns/in-d/home/ludwigschubert/visualize_channels/diagrams/mixed4a-00006/4-max.jpg">
    <img src="https://cnsviewer2.corp.google.com/cns/in-d/home/ludwigschubert/visualize_channels/diagrams/mixed4a-00240/4-max.jpg">
    <img src="https://cnsviewer2.corp.google.com/cns/in-d/home/ludwigschubert/visualize_channels/diagrams/mixed4a-00453/4-max.jpg">
    <img src="https://cnsviewer2.corp.google.com/cns/in-d/home/ludwigschubert/visualize_channels/diagrams/mixed4a-00492/4-max.jpg">
    <figcaption>Examples from the dataset show us what neurons respond to in practice</figcaption>
  </div>
  <div class="row">
    <img src="https://cnsviewer2.corp.google.com/cns/in-d/home/ludwigschubert/visualize_channels/diagrams/mixed4a-00006/optimized.jpg">
    <img src="https://cnsviewer2.corp.google.com/cns/in-d/home/ludwigschubert/visualize_channels/diagrams/mixed4a-00240/optimized.jpg">
    <img src="https://cnsviewer2.corp.google.com/cns/in-d/home/ludwigschubert/visualize_channels/diagrams/mixed4a-00453/optimized.jpg">
    <img src="https://cnsviewer2.corp.google.com/cns/in-d/home/ludwigschubert/visualize_channels/diagrams/mixed4a-00492/optimized.jpg">
    <figcaption>
      <span>
        Optimization isolates the causes of behavior from mere correlations. 
      </span>
      <span>
        A neuron may not be detecting what you initially thought.
      </span>
    </figcaption>
  </div>
  <div class="row">
    <figcaption>XXX or just curved lines? <br><em>mixed4a, Unit 6</em></figcaption>
    <figcaption>Dog faces or just eyes? <br><em>mixed4a, Unit 240</em></figcaption>
    <figcaption>Clouds or just fluffiness? <br><em>mixed4a, Unit 453</em></figcaption>
    <figcaption>Buildings or just pointy lines on sky? <br><em>mixed4a, Unit 492</em></figcaption>
  </div>
</figure>

<p>
  Optimization can also be much more flexible.
  For example, if we want to study how neurons mutually represent information,
  we can easily ask how a particular example would need to be different for an additional neuron to activate.
  This flexibility also allows us to visualize how features evolve as the network trains.
  If we were limited to understanding the model on the fixed examples in our dataset, topics like these ones would be much harder to explore.
</p>

<p>
  Of course, there are also significant challenges to visualizing features with optimization.
</p>


<!-- =================================================== -->

<h2>Diversity</h2>
<p><i>
  Optimization tends to show only one facet of a feature.<br>
  If we're clever it will show us more.
</i></p>
<br>

<!-- TODO: improve las sentence -->
<p>
  Do our examples show us the full picture? 
  When we create examples by optimization, this is something we need to be very careful of.
  It's entirely possible for genuine examples to still mislead us by only showing us one facet of the many things a feature represents.
</p>

<p>
  Dataset examples have a big advantage here.
  By looking through our dataset, we can find diverse examples.
  It doesn't just give us ones activating a neuron intensely:
  we can look across a whole spectrum of activations to see what activates the neuron to different extents.
</p>

<!-- <figure class="l-page-outset"><img src="images/vis_ActivationSpectrum.svg" style="min-height: 150px;"></img></figure> -->


<figure class="l-page-outset">
  <d-figure id="optimization-and-examples"></d-figure>
</figure>


<!-- <figure class="l-page-outset row" id="tmptest">
  <img class="column" src="https://cnsviewer2.corp.google.com/cns/in-d/home/ludwigschubert/visualize_channels/diagrams/mixed4a-00492/0-min.jpg">
  <img class="column" src="https://cnsviewer2.corp.google.com/cns/in-d/home/ludwigschubert/visualize_channels/diagrams/mixed4a-00492/1-some_negative.jpg">
  <img class="column" src="https://cnsviewer2.corp.google.com/cns/in-d/home/ludwigschubert/visualize_channels/diagrams/mixed4a-00492/2-zero.jpg">
  <img class="column" src="https://cnsviewer2.corp.google.com/cns/in-d/home/ludwigschubert/visualize_channels/diagrams/mixed4a-00492/3-some_positive.jpg">
  <img class="column" src="https://cnsviewer2.corp.google.com/cns/in-d/home/ludwigschubert/visualize_channels/diagrams/mixed4a-00492/4-max.jpg">
</figure> -->
<p>
  In contrast, optimization generally gives us one extremely positive example -- and if we're creative, a very negative example as well. 
  Is there some way that optimization could also give us this diversity?
</p>

<h3>Achieving Diversity with Optimization</h3>

<p>
  As is often the case in visualizing neural networks, this problem was initially recognized and addressed by Nguyen and collaborators.
  Their initial approach was to search through the dataset for diverse examples and use those as starting points for the optimization process <d-cite key="nguyen2016multifaceted"></d-cite>.
  The idea is that this initiates optimization in different facets of the feature so that the resulting example from optimization will demonstrate that facet.
  In more recent work, they combine visualizing classes with a generative model, which they can sample for diverse examples <d-cite key="nguyen2016plug"></d-cite>.
</p>

<p>
  The first approach had limited success, and the generative model approach -- which we'll discuss in more depth later -- is a bit tricky and has drawbacks. 
</p>

<p>
  We find there's a very simple pure optimization approach by adding a diversity term to one's objective.
  In lower level neurons, this can reveal the different facets a feature represents:
</p>

<figure class="example-vis2 l-page" style="display: flex; flex-flow: row-wrap;">
  <div style="width: 624px; margin-right: 26px;">
    <img style="width: 624px;" src="images/mixed4a_97_diversity.png" />
    <figcaption>
      Four different, curvy facets of layer mixed4a, unit 97.
    </figcaption>
  </div>
  <div style="width: 147px; ">
    <img style="width: 147px;" src="images/mixed4a_97_dataset_max.jpg" />
    <figcaption>
      Dataset examples
    </figcaption>
  </div>
</figure>

<p>
  The diversity term can take a variety of forms, and we don't have much understanding of their benefits yet.
  One possibility is to penalize the cosine similarity of different examples.
  <d-footnote>
    
  </d-footnote>
  Another is to use ideas from style transfer <d-cite key="gatys2015neural"></d-cite> to force the feature to be displayed in different styles.
</p>


<!-- TODO(colah): Add example where they are very different. -->
<!-- TODO(colah, moralex): Should we make real data diversity available here? -->

<p>
  Diverse feature visualizations allow us to more closely pinpoint what activates a neuron, to the degree that we can make, and -- by looking at dataset examples -- <em>check</em> predictions about what inputs will activate the neuron.
  Take this optimization of layer 4a, unit 143:
</p>

<figure class="l-page-outset" style="display: flex; flex-flow: row-wrap; justify-content: space-between;">
  <div style="width: 147px;">
    <figcaption></figcaption>
    <img style="width: 147px;" src="images/mixed4a_143_optimized.png" />
    <figcaption>
      <strong>Optimization</strong>: Optimizing only the channel shows what looks like red-brown fur, eyes, and rounded shapes.
    </figcaption>
  </div>
  <div style="width: 624px;">
    <figcaption></figcaption>
    <img style="width: 624px; height: 147px;" src="images/mixed4a_143_diversity.png" />
    <figcaption>
      <strong>Adding Diversity</strong>: Optimizing for diversity shows many different features. The only feature common to all visualizations is the red-brown fur. Note that eyes, for example, only appear in the third visualization.
    </figcaption>
  </div>
  <div style="width: 147px; ">
    <figcaption></figcaption>
    <img style="width: 147px;" src="images/mixed4a_143_dataset_max.jpg" />
    <figcaption>
      <strong>Dataset Examples</strong>: As predicted, the channel activates strongly on images of red-brown fur.
    </figcaption>
  </div>
</figure>

<p>
  The effect of diversity can be even more striking in higher level neurons, where it can show us different types of objects that stimulate a neuron.
  For example, one neuron seems responds to people's shoulders and arms.
</p>

<figure class="example-vis2 l-page">
  <div class="chunk">
    <img class="big" src="images/ball_5.jpeg">
    <figcaption>Layer mixed5a, Unit 194</figcaption>
    <!--<figcaption>Facet 2 (score: 1264)</figcaption>-->
  </div>
  <div class="chunk">
    <img class="big" src="images/ball_0.jpeg">
    <!--<figcaption>Facet 1 (score: 1085)</figcaption>-->
  </div>
  <div class="chunk">
    <img class="big" src="images/ball_2.jpeg">
    <!--<figcaption>Facet 3 (score: 998)</figcaption>-->
  </div>
  <div class="chunk">
    <img class="big" src="images/ball_4.jpeg">
    <!--<figcaption>Facet 4 (score: 899)<br><i>layer mixed5a, Unit 194</i></figcaption>-->
  </div>
</figure>

<figure class="example-vis2 l-page">
  <div class="chunk">
    <img class="big" src="images/zookeeper_3.jpeg">
    <!--<figcaption>Facet 2 (score: )</figcaption>-->
    <figcaption>Layer mixed4e, Unit 718</figcaption>
  </div>
  <div class="chunk">
    <img class="big" src="images/zookeeper_1.jpeg">
    <!--<figcaption>Facet 1 (score: )</figcaption>-->
  </div>
  <div class="chunk">
    <img class="big" src="images/zookeeper_4.jpeg">
    <!--<figcaption>Facet 3 (score: )</figcaption>-->
  </div>
  <div class="chunk">
    <img class="big" src="images/zookeeper_5.jpeg">
    <!--<figcaption>Facet 4 (score: )<br><i>layer mixed4e, Unit 718</i></figcaption>-->
  </div>
</figure>

<p>
  While the examples above mostly represent a coherent idea, there are also neurons that represent strange mixtures of ideas.
  For example, below a neuron responds both to billiard tables and cats.
</p>

<figure>
  <!-- images of weird diversity -->
</figure>

<p>
  Examples like these suggest that neurons are not necessarily the right semantic units for understanding NNs.
</p>

<h2>Interaction between Neurons</h2>

<p>
  If neurons are not the right way to understand neural networks, what is?
  In real life, combinations of neurons work together to represent images in neural networks. 
  Individual neurons are the basis directions of activation space, and it is not clear that these should be any more special than any other direction.
</p>

<p>
  In fact, <d-cite key="szegedy2013intriguing">Szegedy <i>et al.</i></d-cite> found that random directions seem just as meaningful as the basis directions.
  We observe the same phenomenon: when we visualize random linear combinations of neurons, the resulting visualization also seem to carry meaning:
</p>

<figure>
  <!-- random direction optimizations; dataset examples? -->
</figure>

<p>  
  We can also define interesting directions (in activatino space) by doing arithmetic on neurons. (TODO:correct?)
  For exampe, if we subtract a "plants" neuron from a "backyard" neuron, we are left with only buildings/fences.
  This is reminiscient of semantic arithmetic of word embeddings as seen in Word2Vec or generative models' latent spaces.
</p>

<figure class="l-text">
  <d-figure id="linear-combinations"></d-figure>
</figure>

<p>
  These examples show us how neurons jointly represent images.
  To better understand how neurons interact, we can interpolate between them. <d-footnote>TODO: shared input parameterization</d-footnote>
  This is similar to interpolating in the latent space of generative models. 
</p>

<figure>
  <!-- 1D interpolation, also optvis, but needs porting -->
</figure>

<p>
  This is only starting to scratch the surface of how neurons interact. 
  The truth is that we have almost no clue how to select meaningful directions, or whether there even exist particularly meaningful directions.
  Independant of finding directions, there are also questions on how directions interact--for example, interpolation can show us how a small number of directions interact, but in reality there are hundreds of directions interacting.
</p>


<!-- =================================================== -->
<h2>The Enemy of Feature Visualization</h2>
<p><i>
  Naively optimizing images doesn't work.<br>
  Regularization is necessary, but cuts both ways.
</i></p>




<h3>High Frequency Patterns</h3>  

<p>
  If you want to visualize features, you might just optimize an image to make neurons fire.
  Unfortunately, this doesn't really work.
  Instead, you end up with a kind of neural network optical illusion
  -- an image full of noise and nonsensical high-frequency patterns that the network responds strongly to.
</p>

<figure id="optimize-naive" class="l-page-outset" style="min-height: 240px;"></figure>

<!-- TODO: Add caption encouraging user to fiddle with learning rate -->

<p>
  These patterns seem to be the images kind of cheating, finding ways to activate neurons that don't occur in real life.
  If you optimize long enough, you'll tend to see some of what the neuron genuinely detects as well,
  but the image is dominated by these high frequency patterns.
  These patterns seem to be closely related to the phenomenon of adversarial counterexamples <d-cite key="szegedy2013intriguing"></d-cite>.
</p>

<p>
  We don't fully understand why these high frequency patterns form,
  but an important part seems to be strided convolutions and pooling operations, which create high-frequency patterns in the gradient <d-cite key="odena2016deconvolution"></d-cite>.
</p>

<figure id="frequency-artifacts" class="l-page-outset"></figure>


<p>
  These high-frequency patterns show us that, while optimization based visualization's freedom from constraints is appealing, it's a double-edged sword.
  Without any constraints on images, we end up with adversarial counterexamples.
  These are certainly interesting, but if we want to understand how these models work in real life, we need to somehow move past them...
</p>

<h3>The Spectrum of Regularization</h3>

<p>
  Dealing with these high frequency noise has been one of the primary challenges and overarching threads of feature visualization research.
  If you want to get useful visualizations, you need to impose a more natural structure using some kind of prior, regularizer, or constraint.
</p>

<p>
  In fact, if you look at most notable papers on feature visualization, some approach to regularization will be one of their main points.
  Researchers have tried a lot of different things!
</p>

<p>
  We can think of all of these approaches as living on a spectrum, based on how strongly they regularize the model.
  On one extreme, if we don't regularize at all, we end up with adversarial counterexamples.
  On the opposite end, we search over examples in our dataset and run into all the limitations we discussed earlier.
  And in the middle we have three main families of regularization options.
</p>

<figure id="feature-vis-history" class="l-page-outset" style="min-height: 750px; padding-bottom: 10px;"></img></figure>


<h3>Three Families of Regularization</h3>

<p>
  Let's consider these three intermediate categories of regularization in more depth.
</p>

<p>
  <b>Frequency penalization</b> directly targets the high frequency noise these methods suffer from.
  It may explicitly penalize variance between neighboring pixels (total variation) <d-cite key="mahendran2015understanding"></d-cite>, or implicitly penalize high-frequency noise by blurring the image each optimization step <d-cite key="nguyen2015deep"></d-cite>.<d-footnote>
    If we think about blurring in Fourier space, it is equivalent to adding a scaled L2 penalty to the objective, penalizing each Fourier-component based on its frequency.</d-footnote>
  Unfortunately, these approaches also discourage legitimate high-frequency features like edges along with noise.
  This can be slightly improved by using a bilateral filter, which preserves edges, instead of blurring <d-cite key="tyka2016bilateral"></d-cite>.
</p>

<p>
  (Some work uses similar techniques to reduce high frequencies in the gradient before they accumulate in the visualization <d-cite key="oygard2015vis,mordvintsev2016deepdreaming"></d-cite>.
  These techniques are in some ways very similar to the above and in some ways radically different -- we'll talk about them in much more depth later.)
</p>

<figure id="regularizer-playground-freq" class="l-page-outset" style="min-height: 240px;"></figure>
<!--
  - none
  - Total variance
  - Blur
-->


<p>
  <b>Transformation robustness</b> tries to find examples that still work if we slightly transform them.
  Even a small amount seems to be very effective in the case of images <d-cite key="mordvintsev2015inceptionism"></d-cite>, 
  especially when combined with a more general regularizer for high-frequencies <d-cite key="oygard2015vis,mordvintsev2016deepdreaming"></d-cite>.
  This usually means that we "jitter" the image, applying the model at slight random offsets every optimization step,
  but other kinds of transformations also help.
</p>


<!--<figure id="optimize-jitter" class="l-page-outset"></figure>-->

<figure id="regularizer-playground-robust" class="l-page-outset" style="min-height: 240px;"></figure>

<!--TODO(colah): Make a diagram of unconstraint opt vs paramaterized opt vs prior? Discuss using an inverse model?-->

<!-- technically most challenging -->

<p>
  <b>Learned priors.</b>
  Our previous regularizers use very simple heuristics to keep examples reasonable.
  A natural next step is to actually learn a model of the real data and try to enforce that.
  With a strong model, this becomes similar to searching over the dataset.
  This approach produces the most photorealistic visualizations, but it may be unclear what came from the model being visualized and what came from the prior.
</p>

<p>
  One approach is to learn a generator that maps points in a latent space to examples of your data,
  such as a GAN or VAE,
  and optimize within that latent space <d-cite key="nguyen2016synthesizing"></d-cite>. 
  An alternative approach is to learn a prior that gives you access to the gradient of probability;
  this allows you to jointly optimize for the prior along with your objective <d-cite key="nguyen2016plug,mordvintsev2015inceptionism"></d-cite>.
  When one optimizes for the prior and the probability of a class, one recovers a generative model of the data conditioned on that particular class.
</p>



<!-- =================================================== -->
<h2>Preconditioning and Parameterization</h2>
<p><i>
  Not all "regularizers" are really regularizers.<br>
  What if the optimization problem is genuinely hard?
</i></p>

<br>

<p>
  In the previous section, we saw a few methods <d-cite key="oygard2015vis,mordvintsev2016deepdreaming"></d-cite> that reduced high frequencies <i>in the gradient</i> rather than the visualization itself.
  It's not clear this is really a regularizer:
  it resists high frequencies, but still allows them to form when the gradient consistently pushes for it.
</p>

<p>
  Transforming the gradient like this is a powerful tool.
  Optimizers call it preconditioning.
  You can think of it as doing steepest descent to optimize the same objective, 
  but in another parameterization of the space or under a different notion of distance.<d-footnote>
    Gradient blurring<d-cite key="oygard2015vis"></d-cite> is equivalent to gradient descent in a different paramaterization of image space, where high frequency dimensions are stretched to make moving in those directions slower. Gradient Laplacian Pyramid normalization <d-cite key="mordvintsev2016deepdreaming"></d-cite> is some kind of adaptive learning rate approach in the same space.
  </d-footnote>
  It changes how fast you move in each direction, but doesn't change what the minimums are.
  Using the right preconditioner can make an optimization problem radically easier.
  And if there are many local minima, it can stretch and shrink their basins of attraction, changing which ones you fall into.
</p>
<p>
  Of course, there are infinitely many possible preconditioners.
  How should we chose a good one that will give us these benefits?
  A good first guess is one that makes your data decorrelated and whitened.<d-footnote>
    In the case of images, this means doing gradient descent in the Fourier transform,
    with frequencies scaled so that they all have equal energy. This decorelates images spatially, but doesn't handle correlations between colors. 
    To address this, we explicitly measure the correlation between colors in the training set and use a Cholesky decomposition to decorelate them.</d-footnote>
  Let's compare the direction of steepest descent in a decorelated parameterization to two other directions of steepest descent:
</p>

<figure id="steepest-descent" class="l-page-outset" style="/*min-height: 290px;*/"></figure>

<p>
  All of these directions are valid descent directions for the same objective,
  but we can see they're radically different.
  Notice that optimizing in the decorrelated space reduces high frequencies,
  while using L<sup>∞</sup> increases them. 
</p>

<p>
  Using the decorrelated descent direction results in quite different visualizations.
  It's hard to do really fair comparisons because of hyperparameters, but the
  resulting visualizations seem a lot better -- and develop faster, too.
</p>

<!-- Figure out which descent directions to have in playground, explain them in the text. -->
<figure id="opt-explore2" class="l-page-outset"></figure>

<p>
  Is the preconditioner merely accelerating descent, bringing us to the same place
  normal gradient descent would have brought us if we were patient enough?
  Or is it also regularizing, changing which local minima we get attracted to?
  It's hard to tell for sure.
  On the one hand, gradient descent seems to continue improving as you exponentially increase the number of optimization steps -- it hasn't converged, it's just moving very slowly.
  On the other hand, if you turn off all other regularizers, the preconditioner seems to reduce high-frequency patterns.
</p>

<h2>Conclusion</h2>

<p>
  Neural feature visualization has made great progress over the last few years.
  As a community, we've developed principled ways to create compelling visualizations. 
  We've mapped out a number of important challenges and found ways of a addressing them.
</p>

<p>
  In the quest to make neural networks interpretable, feature visualization
  stands out as one of the most promising and developed research directions.
  By itself, feature visualization will never give a completely satisfactory
  understanding, but we see it as one of the fundamental building blocks that,
  combined with other tools, will empower humans to understand these systems.
</p>

<p>
  There remains much work to do. 
</p>


</d-article>

<d-appendix>
  <h3>Acknowledgments</h3>
  <p>
    We are extremely grateful to Shan Carter for thoughtful feedback -- especially design advice.
  </p>
  
  <p>
    We're also grateful for the comments, thoughts and support of
    Greg Corrado, Blaise Aguera y Arcas,
    Katherine Ye, Michael Nielsen, Dario Amodei, Ian Goodfellow,
    Mike Tyka,
    Been Kim, Martin Wattenberg, and Fernanda Viegas.
  </p>

  <h3>Author Contributions</h3>
  <p>
    The biggest technical contribution of this work is likely the section on preconditioning.
    Alex discovered in prior work that normalizing gradient frequencies had a radical effect on visualizing neurons.
    Chris reframed this as adaptive gradient descent in a different basis.
    Together, they iterated on a number of ways of paramaterizing images.
    Similarly, Alex originally introduced the use of diversity term, and Chris refined using it.
    Chris did the exploration of interpolating between neurons.
    All experiments were based on code written by Alex.
  </p>
  <p>
    We see another important part of this work as its expository value.
    Chris did most of the distillation of prior work and made most of the interactive diagrams.
    Ludwig created the visualization of the spectrum of neuron activations.
  </p>
  <d-bibliography src="bibliography.bib"></d-bibliography>
</d-appendix>

